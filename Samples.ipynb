{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook used for generating sample data for FID comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pytorch-fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_subset(src, dst, size, count=None):\n",
    "    src = Path(src)\n",
    "    dst = Path(dst)\n",
    "    dst.mkdir(parents=True, exist_ok=True)\n",
    "    objs = sorted(os.listdir(src))\n",
    "    if count is not None:\n",
    "        objs = np.random.choice(objs, count, replace=False)\n",
    "    for o in objs:\n",
    "        img = cv2.imread(str(src/o))\n",
    "        img = cv2.resize(img, size)\n",
    "        cv2.imwrite(str(dst/o), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_all(folder, width, height):\n",
    "    src = Path(folder)\n",
    "    for o in os.listdir(folder):\n",
    "        img = cv2.imread(str(src/o))\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        cv2.imwrite(str(src/o), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize_all('../data/CelebA/val/faces/', 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment below to generate a random sample set of ground truth faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_subset('../data/CelebA/train/faces/', '../data/Samples/ground_truth_faces', (256, 256), count=16384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment below to generate two random ground truth datasets of paprika style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_subset('../data/CelebA/train/paprika', '../data/Samples/ground_truth_paprika1', (256, 256), count=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_subset('../data/CelebA/train/paprika', '../data/Samples/ground_truth_paprika2', (256, 256), count=16384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment below to generate two random subsets of webtoon style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_subset('../data/CelebA/train/webtoon', '../data/Samples/ground_truth_webtoon1', (256, 256), count=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_subset('../data/CelebA/train/webtoon', '../data/Samples/ground_truth_webtoon2', (256, 256), count=16384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment below to generate two random subsets of face v2 style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_subset('../data/CelebA/train/face_v2', '../data/Samples/ground_truth_face_v2_1', (256, 256), count=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_subset('../data/CelebA/train/face_v2', '../data/Samples/ground_truth_face_v2_2', (256, 256), count=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve, urlcleanup\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import albumentations as albm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Flip values for slower training speed, but more determenistic results.\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    torch.cuda.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_subset(model, src, dst, size, count=None):\n",
    "    model = model.to(DEVICE)\n",
    "    src = Path(src)\n",
    "    dst = Path(dst)\n",
    "    dst.mkdir(parents=True, exist_ok=True)\n",
    "    objs = sorted(os.listdir(src))\n",
    "    if count is not None:\n",
    "        objs = np.random.choice(objs, count, replace=False)\n",
    "    for o in objs:\n",
    "        img = cv2.imread(str(src/o))[:, :, ::-1]\n",
    "        img = cv2.resize(img, size)\n",
    "        imageT = torchvision.transforms.ToTensor()(Image.fromarray(img)).unsqueeze(0).to(DEVICE)\n",
    "        output = np.uint8(model(imageT).squeeze(0).detach().permute(1, 2, 0).cpu().numpy() * 255.)\n",
    "        cv2.imwrite(str(dst/o), output[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to replace Mobilenet's ReLU6 activations with ReLU. \n",
    "# There is no noticeable difference in quality, but this will\n",
    "# allow us to use CoreML for mobile inference on iOS devices.\n",
    "def replace_relu6_with_relu(model):\n",
    "    for name, module in reversed(model._modules.items()):\n",
    "        if len(list(module.children())) > 0:\n",
    "            model._modules[name] = replace_relu6_with_relu(model=module)\n",
    "        if isinstance(module, nn.ReLU6):\n",
    "            model._modules[name] = nn.ReLU()\n",
    "    return model\n",
    "\n",
    "\n",
    "class AnimeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        mobilenet = torchvision.models.mobilenet_v2(width_mult=0.5)\n",
    "\n",
    "        # We reuse state dict from mobilenet v2 width width_mult == 1.0.\n",
    "        # This is not the optimal way to use pretrained models, but in this case\n",
    "        # it gives us good initialization for faster convergence.\n",
    "        state_dict = torchvision.models.mobilenet_v2(pretrained=True).state_dict()\n",
    "        target_dict = mobilenet.state_dict()\n",
    "        for k in target_dict.keys():\n",
    "            if len(target_dict[k].size()) == 0:\n",
    "                continue\n",
    "            state_dict[k] = state_dict[k][:target_dict[k].size(0)]\n",
    "            if len(state_dict[k].size()) > 1:\n",
    "                state_dict[k] = state_dict[k][:, :target_dict[k].size(1)]\n",
    "\n",
    "        mobilenet.load_state_dict(state_dict)\n",
    "\n",
    "        weight = mobilenet.features[0][0].weight.detach()\n",
    "        # mobilenet.features[0][0].weight = nn.Parameter(data=weight / 255.)\n",
    "\n",
    "        mobilenet = replace_relu6_with_relu(mobilenet)\n",
    "\n",
    "        self.features = mobilenet.features[:-2]\n",
    "        self.upscale0 = nn.Sequential(\n",
    "            nn.Conv2d(80, 48, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale1 = nn.Sequential(\n",
    "            nn.Conv2d(48, 16, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 8, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale4 = nn.Sequential(\n",
    "            nn.Conv2d(8, 4, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale5 = nn.Conv2d(4, 3, 3, 1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        skip_outs = []\n",
    "        for i in range(len(self.features)):\n",
    "            out = self.features[i](out)\n",
    "            if i in {1, 3, 6, 13}:\n",
    "                skip_outs.append(out)\n",
    "        out = self.upscale0(out)\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale1(out + skip_outs[3])\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale2(out + skip_outs[2])\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale3(out + skip_outs[1])\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale4(out + skip_outs[0])\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale5(out)\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAnimeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        mobilenet = torchvision.models.mobilenet_v2(width_mult=0.75)\n",
    "\n",
    "        # We reuse state dict from mobilenet v2 width width_mult == 1.0.\n",
    "        # This is not the optimal way to use pretrained models, but in this case\n",
    "        # it gives us good initialization for faster convergence.\n",
    "        state_dict = torchvision.models.mobilenet_v2(pretrained=True).state_dict()\n",
    "        target_dict = mobilenet.state_dict()\n",
    "        for k in target_dict.keys():\n",
    "            if len(target_dict[k].size()) == 0:\n",
    "                continue\n",
    "            state_dict[k] = state_dict[k][:target_dict[k].size(0)]\n",
    "            if len(state_dict[k].size()) > 1:\n",
    "                state_dict[k] = state_dict[k][:, :target_dict[k].size(1)]\n",
    "\n",
    "        mobilenet.load_state_dict(state_dict)\n",
    "\n",
    "        weight = mobilenet.features[0][0].weight.detach()\n",
    "\n",
    "        mobilenet = replace_relu6_with_relu(mobilenet)\n",
    "\n",
    "        self.features = mobilenet.features[:-2]\n",
    "        self.upscale0 = nn.Sequential(\n",
    "            nn.Conv2d(120, 72, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(72),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale1 = nn.Sequential(\n",
    "            nn.Conv2d(72, 24, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale2 = nn.Sequential(\n",
    "            nn.Conv2d(24, 24, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale3 = nn.Sequential(\n",
    "            nn.Conv2d(24, 16, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale4 = nn.Sequential(\n",
    "            nn.Conv2d(16, 4, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale5 = nn.Conv2d(4, 3, 3, 1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        skip_outs = []\n",
    "        for i in range(len(self.features)):\n",
    "            out = self.features[i](out)\n",
    "            if i in {1, 3, 6, 13}:\n",
    "                skip_outs.append(out)\n",
    "        out = self.upscale0(out)\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale1(out + skip_outs[3])\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale2(out + skip_outs[2])\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale3(out + skip_outs[1])\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale4(out + skip_outs[0])\n",
    "        out = nn.functional.interpolate(out, scale_factor=2, mode='nearest')\n",
    "        out = self.upscale5(out)\n",
    "        return torch.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnimeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPRIKA = 'paprika.pth'\n",
    "FACE_V2 = 'face_v2.pth'\n",
    "WEBTOON = 'webtoon.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('mobilenetv2_256_sobel2_anime.pth'))\n",
    "model.load_state_dict(torch.load('face_v2_2.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a dataset from our trained model for evaluation (using FID) against ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_subset(model, '../data/Samples/ground_truth_faces', '../data/Samples/face_v2_3', (256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
